{
    "nbformat_minor": 0, 
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "pygments_lexer": "ipython2", 
            "version": "2.7.11", 
            "name": "python", 
            "file_extension": ".py", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python2-spark20", 
            "display_name": "Python 2 with Spark 2.0"
        }
    }, 
    "cells": [
        {
            "execution_count": 6, 
            "source": "\nfrom pyspark.sql import SparkSession\n\ndef set_hadoop_config_with_credentials_216c032f3f574763ae975c6a83a0d523(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', 'e097bbd898534ed1ad0e45c82baedb2d')\n    hconf.set(prefix + '.username', '9a3f2edd83394c798350768f9e1582db')\n    hconf.set(prefix + '.password', 'XXXXXXXX')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_216c032f3f574763ae975c6a83a0d523(name)\n\nspark = SparkSession.builder.getOrCreate()\n\n# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n# The SparkSession object is already initalized for you.\n# The following variable contains the path to your file on your Object Storage.\npath_1 = \"swift://testSchedulling.\" + name + \"/co2.csv.gz\"\n", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": 7, 
            "source": "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true',codec=\"org.apache.hadoop.io.compress.GzipCodec\").load(path_1)", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": 8, 
            "source": "df.printSchema()", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "root\n |-- CO2 per capita: string (nullable = true)\n |-- Year: integer (nullable = true)\n |-- value: double (nullable = true)\n\n"
                }
            ]
        }, 
        {
            "execution_count": null, 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat": 4
}