{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<h1>Simple Kafka Consumer for Spark Streaming to consume topics from bluemix message hub service.</h1>"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<h4>This example uses IBM CDS's spark example jar. https://github.com/DTAIEB/demos/raw/master/streaming-twitter-assembly-1.6.jar </h4>"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<h4>Prerequisites:- This assumes you have your kafka(IBM Bluemix Message Hub) service running and you have created topics and have\na running producer which produces messages to Topic(queue) \"topicJJ\". You can easily follow this documentation and get a producer started from your local machine to test this.\nhttps://console.ng.bluemix.net/docs/services/MessageHub/index.html\n</h4>"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<h4>One more prerequisite needed is to add the library jar to your spark packages to use it. We will use pixiedustmanager to install\nthe jar package. First switch to python kernel 2.0 or 1.6. If you already don't have pixiedust installed, run '!pip install --upgrade pixiedust' to install it. Run first two cells of installation in Python and then change kernel to scala for streaming</h4>"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "text": "Pixiedust database opened successfully\nPixiedust version 0.75\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "import pixiedust", 
            "execution_count": 1
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "pixiedust.installPackage(\"https://github.com/DTAIEB/demos/raw/master/streaming-twitter-assembly-1.6.jar\")", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "<h4>Create a connection to kafka service from bluemix and do insert to code or simply replace below credentials with your\nkafka brokers , username and password, this three parameters we will need later. Note i have added topic parameter.\n</h4>"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "// The code was removed by DSX for sharing.", 
            "execution_count": 1
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "import com.ibm.cds.spark.samples.config.MessageHubConfig", 
            "execution_count": 2
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.streaming.Duration\nimport org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\nimport com.ibm.cds.spark.samples.config.MessageHubConfig\nimport com.ibm.cds.spark.samples.dstream.KafkaStreaming.KafkaStreamingContextAdapter\n\nimport org.apache.kafka.common.serialization.Deserializer\nimport org.apache.kafka.common.serialization.StringDeserializer", 
            "execution_count": 3
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "text": "default location of ssl Trust store is: /usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/security/cacerts\ndefault location of ssl Trust store is: /usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/security/cacerts\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "val kafkaProps = new MessageHubConfig\n\nkafkaProps.setConfig(\"bootstrap.servers\",message_hub(\"kafka_brokers_sasl\"))\nkafkaProps.setConfig(\"kafka.user.name\",message_hub(\"user\"))\nkafkaProps.setConfig(\"kafka.user.password\",message_hub(\"password\"))\nkafkaProps.setConfig(\"kafka.topic\",message_hub(\"password\"))\n\n\nval ssc = new StreamingContext( sc, Seconds(5) )\n\nval stream = ssc.createKafkaStream[String, String, StringDeserializer, StringDeserializer](kafkaProps,List(message_hub(\"topic\")));\n\nstream.print()", 
            "execution_count": 4
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<h5>At this point you can start your producer or start it after the consumer, here is sample output</h5>\n<code>\n$ java -jar build/libs/kafka-java-console-sample-2.0.jar 'kafka04-prod01.messagehub.services.us-south.bluemix.net:9093' 'https://kafka-admin-prod01.messagehub.services.us-south.bluemix.net:443' 'XXXXXXXXXXX' -topic topicJJ -producer\n[2017-03-22 16:35:05,994] INFO Running in local mode. (com.messagehub.samples.MessageHubConsoleSample)\n[2017-03-22 16:35:05,994] INFO Updating JAAS configuration (com.messagehub.samples.MessageHubConsoleSample)\n[2017-03-22 16:35:06,008] INFO Kafka Endpoints: kafka04-prod01.messagehub.services.us-south.bluemix.net:9093 (com.messagehub.samples.MessageHubConsoleSample)\n[2017-03-22 16:35:06,008] INFO Admin REST Endpoint: https://kafka-admin-prod01.messagehub.services.us-south.bluemix.net:443 (com.messagehub.samples.MessageHubConsoleSample)\n[2017-03-22 16:35:06,008] INFO Creating the topic topicJJ (com.messagehub.samples.MessageHubConsoleSample)\n (com.messagehub.samples.MessageHubConsoleSample)e :{\"errorCode\":42201,\"errorMessage\":\"Topic \\\"topicJJ\\\" already exists.\"}\n[2017-03-22 16:35:07,742] INFO Admin REST Listing Topics: [{\"name\":\"kafka-java-console-sample-topic\",\"partitions\":1,\"retentionMs\":\"86400000\",\"markedForDeletion\":false},{\"name\":\"topicJJ\",\"partitions\":5,\"retentionMs\":\"86400000\",\"markedForDeletion\":false}] (com.messagehub.samples.MessageHubConsoleSample)\n[2017-03-22 16:35:08,606] INFO [Partition(topic = topicJJ, partition = 2, leader = 8, replicas = [1,2,8,], isr = [8,1,2,], Partition(topic = topicJJ, partition = 4, leader = 1, replicas = [1,3,4,], isr = [1,3,4,], Partition(topic = topicJJ, partition = 1, leader = 7, replicas = [0,1,7,], isr = [7,0,1,], Partition(topic = topicJJ, partition = 3, leader = 0, replicas = [0,2,3,], isr = [0,2,3,], Partition(topic = topicJJ, partition = 0, leader = 6, replicas = [0,6,8,], isr = [6,8,0,]] (com.messagehub.samples.ProducerRunnable)\n[2017-03-22 16:35:08,606] INFO MessageHubConsoleSample will run until interrupted. (com.messagehub.samples.MessageHubConsoleSample)\n[2017-03-22 16:35:08,606] INFO class com.messagehub.samples.ProducerRunnable is starting. (com.messagehub.samples.ProducerRunnable)\n[2017-03-22 16:35:09,213] INFO Message produced, offset: 3729 (com.messagehub.samples.ProducerRunnable)\n[2017-03-22 16:35:11,292] INFO Message produced, offset: 3730 (com.messagehub.samples.ProducerRunnable)\n[2017-03-22 16:35:13,359] INFO Message produced, offset: 3731 (com.messagehub.samples.ProducerRunnable)\n^Z\n[6]+  Stopped                 java -jar build/libs/kafka-java-console-sample-2.0.jar 'kafka04-prod01.messagehub.services.us-south.bluemix.net:9093' 'https://kafka-admin-prod01.messagehub.services.us-south.bluemix.net:443' 'KcaekYK8fMHhRhv6gqr32p0PAOGgHeg9455wINaNeKZaJNK0' -topic topicJJ -producer\ncharless-MacBook-Pro-3:kafka-java-console-sample charles$\n</code>"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<h5>Now start the sparkstreaming consumer to read the streaming messages from kafka topic</h5>"
                    }, 
                    "execution_count": 7, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "<h5>Now start the sparkstreaming consumer to read the streaming messages from kafka topic</h5>", 
            "execution_count": 7
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "text": "-------------------------------------------\nTime: 1490225700000 ms\n-------------------------------------------\n\n-------------------------------------------\nTime: 1490225705000 ms\n-------------------------------------------\n\n-------------------------------------------\nTime: 1490225710000 ms\n-------------------------------------------\n(key,This is a test message #0)\n\n-------------------------------------------\nTime: 1490225715000 ms\n-------------------------------------------\n(key,This is a test message #1)\n(key,This is a test message #2)\n\n-------------------------------------------\nTime: 1490225720000 ms\n-------------------------------------------\n\n-------------------------------------------\nTime: 1490225725000 ms\n-------------------------------------------\n\n-------------------------------------------\nTime: 1490225730000 ms\n-------------------------------------------\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "source": "ssc.start()", 
            "execution_count": 5
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "<h5>Use Ctrl+C to stop the producer if you have it running and to stop consumer(sparkstreaming context) use ssc.stop()</h5>", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "ssc.stop()", 
            "execution_count": 6
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "", 
            "execution_count": null
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.10 with Spark 1.6", 
            "name": "scala", 
            "language": "scala"
        }, 
        "language_info": {
            "name": "scala"
        }
    }, 
    "nbformat_minor": 0
}
