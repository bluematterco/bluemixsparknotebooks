{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-02-16 11:00:18--  https://github.com/databricks/spark-avro/raw/branch-3.2/src/test/resources/episodes.avro\n",
      "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
      "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/databricks/spark-avro/branch-3.2/src/test/resources/episodes.avro [following]\n",
      "--2017-02-16 11:00:18--  https://raw.githubusercontent.com/databricks/spark-avro/branch-3.2/src/test/resources/episodes.avro\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 597 [application/octet-stream]\n",
      "Saving to: ‘episodes.avro’\n",
      "\n",
      "100%[======================================>] 597         --.-K/s   in 0s      \n",
      "\n",
      "2017-02-16 11:00:18 (123 MB/s) - ‘episodes.avro’ saved [597/597]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/databricks/spark-avro/raw/branch-3.2/src/test/resources/episodes.avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Failed to find data source: com.databricks.spark.avro. Please find an Avro package at https://cwiki.apache.org/confluence/display/SPARK/Third+Party+Projects;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-facdb624aa3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creates a DataFrame from a specified directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.databricks.spark.avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"episodes.avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#  Saves the subset of the Avro records read in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doctor > 5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark20master/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark20master/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark20master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Failed to find data source: com.databricks.spark.avro. Please find an Avro package at https://cwiki.apache.org/confluence/display/SPARK/Third+Party+Projects;'"
     ]
    }
   ],
   "source": [
    "# Creates a DataFrame from a specified directory\n",
    "df = spark.read.format(\"com.databricks.spark.avro\").load(\"episodes.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change kernel to python 1.6 , you should reset kernel after installation\n",
    "!pip install --user pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n",
      "Pixiedust version 0.75\n"
     ]
    }
   ],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading package com.databricks:spark-avro_2.11:3.0.0 to /gpfs/fs01/user/s7b4-b98406426c07e8-2c631c8ff999/data/libs/spark-avro_2.11-3.0.0.jar\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div>\n",
       "                    <span id=\"pm_labelc5e2fa7c\">Starting download...</span>\n",
       "                    <progress id=\"pm_progressc5e2fa7c\" max=\"100\" value=\"0\" style=\"width:200px\"></progress>\n",
       "                </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "                    $(\"#pm_labelc5e2fa7c\").text(\"Downloaded 8192 of 84277 bytes\");\n",
       "                    $(\"#pm_progressc5e2fa7c\").attr(\"value\", 9.72);\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "                    $(\"#pm_labelc5e2fa7c\").text(\"Downloaded 16384 of 84277 bytes\");\n",
       "                    $(\"#pm_progressc5e2fa7c\").attr(\"value\", 19.44);\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "                    $(\"#pm_labelc5e2fa7c\").text(\"Downloaded 24576 of 84277 bytes\");\n",
       "                    $(\"#pm_progressc5e2fa7c\").attr(\"value\", 29.16);\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "                    $(\"#pm_labelc5e2fa7c\").text(\"Downloaded 32768 of 84277 bytes\");\n",
       "                    $(\"#pm_progressc5e2fa7c\").attr(\"value\", 38.88);\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "                    $(\"#pm_labelc5e2fa7c\").text(\"Downloaded 40960 of 84277 bytes\");\n",
       "                    $(\"#pm_progressc5e2fa7c\").attr(\"value\", 48.6);\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "                    $(\"#pm_labelc5e2fa7c\").text(\"Downloaded 49152 of 84277 bytes\");\n",
       "                    $(\"#pm_progressc5e2fa7c\").attr(\"value\", 58.32);\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "                    $(\"#pm_labelc5e2fa7c\").text(\"Downloaded 57344 of 84277 bytes\");\n",
       "                    $(\"#pm_progressc5e2fa7c\").attr(\"value\", 68.04);\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "                    $(\"#pm_labelc5e2fa7c\").text(\"Downloaded 65536 of 84277 bytes\");\n",
       "                    $(\"#pm_progressc5e2fa7c\").attr(\"value\", 77.76);\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "                    $(\"#pm_labelc5e2fa7c\").text(\"Downloaded 73728 of 84277 bytes\");\n",
       "                    $(\"#pm_progressc5e2fa7c\").attr(\"value\", 87.48);\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "                    $(\"#pm_labelc5e2fa7c\").text(\"Downloaded 81920 of 84277 bytes\");\n",
       "                    $(\"#pm_progressc5e2fa7c\").attr(\"value\", 97.2);\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "                    $(\"#pm_labelc5e2fa7c\").text(\"Downloaded 84277 of 84277 bytes\");\n",
       "                    $(\"#pm_progressc5e2fa7c\").attr(\"value\", 100.0);\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package com.databricks:spark-avro_2.11:3.0.0 downloaded successfully\n",
      "\u001b[31mPlease restart Kernel to complete installation of the new package\u001b[0m\n",
      "Successfully added package com.databricks:spark-avro_2.11:3.0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pixiedust.packageManager.package.Package at 0x7f5e644db890>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixiedust.installPackage(\"com.databricks:spark-avro_2.11:3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Restart Kernel and then switch to spark 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(title=u'The Eleventh Hour', air_date=u'3 April 2010', doctor=11),\n",
       " Row(title=u\"The Doctor's Wife\", air_date=u'14 May 2011', doctor=11),\n",
       " Row(title=u'Horror of Fang Rock', air_date=u'3 September 1977', doctor=4),\n",
       " Row(title=u'An Unearthly Child', air_date=u'23 November 1963', doctor=1),\n",
       " Row(title=u'The Mysterious Planet', air_date=u'6 September 1986', doctor=6)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a DataFrame from a specified directory\n",
    "df = spark.read.format(\"com.databricks.spark.avro\").load(\"episodes.avro\")\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now switch to R kernel to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>title</th><th scope=col>air_date</th><th scope=col>doctor</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>The Eleventh Hour    </td><td>3 April 2010         </td><td>11                   </td></tr>\n",
       "\t<tr><td>The Doctor's Wife    </td><td>14 May 2011          </td><td>11                   </td></tr>\n",
       "\t<tr><td>Horror of Fang Rock  </td><td>3 September 1977     </td><td> 4                   </td></tr>\n",
       "\t<tr><td>An Unearthly Child   </td><td>23 November 1963     </td><td> 1                   </td></tr>\n",
       "\t<tr><td>The Mysterious Planet</td><td>6 September 1986     </td><td> 6                   </td></tr>\n",
       "\t<tr><td>Rose                 </td><td>26 March 2005        </td><td> 9                   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " title & air\\_date & doctor\\\\\n",
       "\\hline\n",
       "\t The Eleventh Hour     & 3 April 2010          & 11                   \\\\\n",
       "\t The Doctor's Wife     & 14 May 2011           & 11                   \\\\\n",
       "\t Horror of Fang Rock   & 3 September 1977      &  4                   \\\\\n",
       "\t An Unearthly Child    & 23 November 1963      &  1                   \\\\\n",
       "\t The Mysterious Planet & 6 September 1986      &  6                   \\\\\n",
       "\t Rose                  & 26 March 2005         &  9                   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  title                 air_date         doctor\n",
       "1 The Eleventh Hour     3 April 2010     11    \n",
       "2 The Doctor's Wife     14 May 2011      11    \n",
       "3 Horror of Fang Rock   3 September 1977  4    \n",
       "4 An Unearthly Child    23 November 1963  1    \n",
       "5 The Mysterious Planet 6 September 1986  6    \n",
       "6 Rose                  26 March 2005     9    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 <- read.df(\"episodes.avro\", source = \"com.databricks.spark.avro\", header = \"true\")\n",
    "head(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R with Spark 2.0",
   "language": "R",
   "name": "r-spark20"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}